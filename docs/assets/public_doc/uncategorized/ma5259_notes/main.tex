\documentclass{report}
\usepackage{graphicx} % Required for inserting images
\input{header}

% start with chapter 0
\setcounter{chapter}{-1}
\renewcommand{\theequation}{\thechapter.\arabic{equation}}

\title{
Probability Theory \\
\large 
\emph{
a rigorous treatment to probability theory and its applications to stochastic processes \\
}
}
\author{Khanh Nguyen}
\date{August 2024}

\begin{document}

\maketitle

\chapter{Preliminaries}

\section{Linear Algebra}

\begin{proposition}
    Let $A \in \R^{n \times n}$, then $A$ and $A^T$ have the same eigenvalues
\end{proposition}

\chapter{Probability Theory}

\section{Minimal measure theory to fulfil this}

\subsection{Measurable Space}

\begin{definition}[$\sigma$-algebra, measurable space]
    Let $X$ be a set. A $\sigma$-algebra $\Sigma$ on $X$ is a collection of subsets of $X$ such that:
    \begin{enumerate}
        \item $\emptyset, X \in \Sigma$
        \item $A \in \Sigma \implies X - A \in \Sigma$
        \item $E_1, E_2, ... \in \Sigma \implies \bigcup_{i=1}^\infty E_i \in \Sigma$
    \end{enumerate}
    The pair $(X, \Sigma)$ is called measurable space, and elements of $\Sigma$ are called measurable set.
\end{definition}

\begin{definition}[$\sigma$-algebra generated by basis]
    Let $X$ be a set and $\mathcal{B}$ be a collection of subsets of $X$. Define $\sigma(B)$ by the smallest $\sigma$-algebra containing $\mathcal{B}$, that is, the intersection of all $\sigma$-algebras containing $\mathcal{B}$. \note{(since intersection of arbitrary collection of $\sigma$-algebras is another $\sigma$-algbera, the defintion is well-defined)}
\end{definition}

\begin{definition}[product of measurable spaces, product $\sigma$-algebra]
    Let $\set{(X_i, \Sigma_i)}_{i \in I}$ be a collection of measurable spaces. Define the product of measurable spaces $(X, \Sigma)$ by
    \begin{align*}
        X       &:= \prod_{i \in I} X_i \\
        \Sigma  &:= \sigma \tuple*{\prod_{i \in I} \Sigma_i}
    \end{align*}
    
    where products are cartesian products. $\Sigma$ is called product $\sigma$-algebra.
\end{definition}

\begin{definition}[measurable function, the category of measurable spaces]
    Let $(X, \Sigma_X), (Y, \Sigma_Y)$ be measurable spaces. A function $f: X \to Y$ is called measurable if for every measurable set $E_Y$ in $(Y, \Sigma_Y)$, the preimage $f^{-1} E_Y$ is measurable in $(X, \Sigma_X)$. The pair measurable space, measurable function form a category called the category of measurable spaces denoted by $\Meas$, the product in this category is precisely the product of measurable spaces.
\end{definition}

\begin{definition}[subspace]
    Let $(X, \Sigma)$ be a measurable space and $A \in \Sigma$ be a measurable set. Then $A$ induces a measurable space $(A, \Sigma_A)$ defined by
    $$
        \Sigma_A = \set{A \cap E: E \in \Sigma}
    $$
\end{definition}

\subsection{Measure Space}

\begin{definition}[measure, measure space]
    Let $(X, \Sigma)$ be a measurable space. A measure $\mu$ on $(X, \Sigma)$ is a function $\mu: X \to [0, +\infty]$ such that
    \begin{enumerate}
        \item $\mu(\emptyset) = 0$
        \item $\mu\tuple*{\coprod_{i=1}^\infty E_i} = \sum_{i=1}^\infty \mu(E_i)$
    \end{enumerate}
    where $\coprod$ denotes the disjoint union. The triplet $(X, \Sigma, \mu)$ is called measure space.
\end{definition}

\begin{definition}[subspace]
    Let $(X, \Sigma, \mu)$ be a measure space and $A \in \Sigma$ be a measurable set. Then $A$ induces a measure space $(A, \Sigma_A, \mu_A)$ defined by
    $$
        \mu_A(E_A) = \mu(E \cap A)
    $$
    where $E_A = E \cap A$
\end{definition}


\begin{definition}[measure-preserving map, the category of measure spaces]
    Let $f: (X, \Sigma_X, \mu_X) \to (Y, \Sigma_Y, \mu_Y)$ be a measurable function, $f$ is called measure preserving-map if
    $$
        \mu_X(f^{-1} E) = \mu_Y (E)
    $$
    for all $E \in \Sigma_Y$. The pair measure space and measure-preserving map form a category called the category of measure space.
\end{definition}

\begin{definition}[pushforward measure]
    Let $f: (X, \Sigma_X) \to (Y, \Sigma_Y)$ be a measurable function, and $\mu_X: \Sigma_X \to [0, +\infty]$ be a measure on $X$. The pushforward measure of $\mu_X$ by $f$ is the unique measure $\mu_Y$ such that $f$ is a measure-preserving map
\end{definition}

\begin{theorem}[change of variables]
    Let $\phi: (X, \Sigma_X, \mu_X) \to (Y, \Sigma_Y, \mu_Y)$ be a measure-preserving map. A measurable function $f: Y \to \R$ is integrable with respect to $\mu_Y$ if and only if the composition $f \phi$ is integrable with respect to $\mu_X$, in that case, the integrals coincide
    $$
        \int_Y \phi d\mu_Y = \int_X f \phi d\mu_X
    $$
    \begin{center}
\begin{tikzcd}
	{(X, \Sigma_X, \mu_X)} \arrow[r, "\phi"] \arrow[rd, "f \phi"', dashed] & {(Y, \Sigma_Y, \mu_Y)} \arrow[d, "f"] \\
	& \R                                   
\end{tikzcd}
    \end{center}
    Equivalently, $\phi$ induces an isomorphism $\phi_*$ in $L^1$ spaces that preserves integral, i.e. $\int_Y f d\mu_Y = \int_X \phi_*(f) d\mu_X$ defined by
    \begin{align*}
        \phi_*: L^1(Y, \Sigma_Y, \mu_Y) &\to L^1(X, \Sigma_X, \mu_X) \\
        f &\mapsto \phi_*(f) = f \phi
    \end{align*}
\end{theorem}
 
\section{Probability Spaces and Random Variables}

\subsection{Probability Space}

\begin{definition}[probability space]
    A probability space $(\Omega, \mathcal{F}, P)$ is a measure space such that $P(\Omega) = 1$. $\Omega$ is called sample space, measurable sets in $\mathcal{F}$ are called events, $P$ is called probability measure.    
\end{definition}

\begin{definition}[independence of events]
    Let $(\Omega, \mathcal{F}, P)$ be a probability space, $\set{E_i}_{i \in I}$ be a collection of events. The collection is called independent if for any finite subcollection $J \subseteq I$
    $$
        P\tuple*{\bigcap_{j \in J} E_j} = \prod_{j \in J} P(E_j)
    $$
\end{definition}

\subsection{Random Variables}

\begin{definition}[pushforward probability space, random variable]
    Let $(\Omega, \mathcal{F}, P)$ be a probability space, $(\mathcal{X}, \mathcal{F}_X)$ be a measurable space, and $X: \Omega \to \mathcal{X}$ be a measurable function. Let $P_X: \mathcal{F}_X \to \R$ be the pushforward measure of $X$, then $(\mathcal{X}, \mathcal{F}_X, P_X)$ is another probability space. $(\mathcal{X}, \mathcal{F}_X, P_X)$ is called pushforward probability space, the measurable function $X$ is called random variable, and the pushforward measure $P_X$ is called probability distribution.
    \begin{align*}
        P_X:    \mathcal{F}_X &\to \R \\
                E_X &\mapsto P(X^{-1} E_X)
    \end{align*}
    
    If $\mathcal{X}$ is the codomain of the random variable $X: \Omega \to \mathcal{X}$, we call $X$ a random variable on $\mathcal{X}$.
\end{definition}

\begin{remark}:
    \begin{itemize}
        \item In probability theory, we usually start with the unique probability space $(\Omega, \mathcal{F}, P)$, namely, abstract probability space, and all random variables are measurable functions from $\Omega$. Denote the collection of all random variables $\Omega \to X$ by 
        $$
            \RV[\mathcal{X}] := \Hom((\Omega, \mathcal{F}), (\mathcal{X}, \mathcal{F}_X))
        $$
        
        \item Without confusion, we identify the two events $E_X \in \mathcal{F}_X$ with $E = X^{-1} E_X \in \mathcal{F}$ and write
        $$
        	P(E_X) := P_X(E_X)
        $$
    \end{itemize}
\end{remark}

\begin{definition}[joint distribution]
    Let $\set{X_i: \Omega \to \mathcal{X}_i}_{i \in I}$ be a collection of random variables. Then, $X: \Omega \to \mathcal{X}$ is a random variable on the product of measurable spaces $\mathcal{X} = \prod_{i \in I} \mathcal{X}_i$ defined by
    $$
        X(\omega) := \prod_{i \in I} X_i(\omega)
    $$
    $X$ is called the joint random variable, the probability distribution on $X$ is called joint distribution.
\end{definition}

\begin{remark}
    Let $\set{X_i: \Omega \to \mathcal{X}_i}_{i \in I}$ be a collection of random variables, and $X$ be the joint random variable. An $\Tilde{E}_j = \prod_{i \in I} E_i$ be an event in $X$ such that $E_i = \mathcal{X}_i$ for all but index $j$, that is, projections of $\Tilde{E}_j$ on all coordinates are the whole space except coordinate $j$. Then, we identify $\Tilde{E}_j$ by $E_j$.
\end{remark}

\begin{definition}[independence of random variables]
    Let $\set{X_i: \Omega \to \mathcal{X}_i}_{i \in I}$ be a collection of random variables, and $X$ be the joint random variable. The collection is called (mutually) independent if every collection of events $\set{E_i \in X_i}_{i \in I}$ is independent.
\end{definition}

\begin{definition}[function on random variables]
    Let $\mathcal{X}, \mathcal{Y}$ be measurable spaces, $X: \Omega \to \mathcal{X}$ be a random variable on $X$. Let $f: \mathcal{X} \to \mathcal{Y}$ be a measurable function. Then, $f$ induces \footnote{composition of measurable functions is measurable} a random variable on $Y$ defined by
    \begin{align*}
        f_*: \RV[\mathcal{X}] &\to \RV[\mathcal{Y}] \\
        X &\mapsto f_* X = f X
    \end{align*}
\end{definition}

\note{Conditioning should be introduced here, however, it is a difficult topic and I did not have enough maturity to write an abstract introduction to conditioning, so I will put conditioning after real-valued random variables. In short, conditioning on an event is to induce new probability measure}

\section{Real-Valued Random Variables}

Assume $\R$ is equipped with the Borel-algebra: the $\sigma$-algbera generated by open sets of the usual topology.

\begin{definition}[real-valued random variable]
    A real-valued random variable $X: \Omega \to \R$ is a random variable on the measurable space $\R$. The collection of real-valued random variables is denoted by $\RV[\R]$
\end{definition}

\begin{proposition}[algebra over field]
    Since $\R$ is a field, $\RV[\R]$ is an algebra over $\R$ with vector addition, and scalar multiplication, vector multiplication are defined by
    \begin{itemize}
        \item \textbf{vector addition}: $(X + Y)(\omega) = X(\omega) + Y(\omega)$
        \item \textbf{scalar multiplication}: $(cX)(\omega) = c X(\omega)$
        \item \textbf{vector multiplication}: $(XY)(\omega) = X(\omega) Y(\omega)$
    \end{itemize}
\end{proposition}

\begin{definition}[distribution function, absolutely continuous random variable]
	Let $X: \Omega \to \R$ be a real-valued random variable, define $F_X: \R \to [0, 1]$ by
	$$
		F_X(a) = P(X \leq a)
	$$
	
	$F_X$ is called distribtion function of random variable $X$. $X$ is called (absolutely) continuous if $F_X$ is an absolutely continuous function. When $X$ is continuous, there exists a $L^1$ function $f_X: \R \to \R$ so that
	$$
		P(X \leq a) = F_X(a) = \int_{-\infty}^a f_X(x) dx
	$$
	
	$f_X$ is called density function.
\end{definition}

From now, whenever we write $f_X$, we assume that $X$ is continuous.

\subsection{Expectation and Variance of Real-Valued Random Variables}

\subsubsection{Expectation of Real-Valued Random Variables}

\begin{definition}[expectation]
    Let $(\Omega, \mathcal{F}, P)$ be a probability space and $X: \Omega \to \mathcal{X} = \R$ be a real-valued random variable. Define the expectation $\Exp[-]: \RV[\R] \to \R$ by
    \begin{align*}
        \Exp[X]
        &:= \int_{\Omega} X dP \\
        &:= \int_{\Omega} (\id X) dP &\text{($\id: \mathcal{X} = \R \to \R$)} \\
        &= \int_\R \id dP_X &\text{(by change of variables w.r.t pushforward $X$)}\\
        &= \int_\R x dP_X(x)
    \end{align*}
\end{definition}

\begin{proposition}[linearity of expectation]
    Expectation is a linear map $\RV[\R] \to \R$. That is,
    \begin{itemize}
        \item $\Exp[X + Y] = \Exp[X] + \Exp[Y]$
        \item $\Exp[cX] = c\Exp[X]$
    \end{itemize}
\end{proposition}

\begin{proposition}[expectation of function on real-valued random variables]
    Let $f: \R \to \R$ be a real-valued measurable function, then
    \begin{align*}
        \Exp[f X]
        &= \int_{\Omega} (f X) dP \\
        &= \int_\R f(x) dP_X(x) &\text{(by change of variables w.r.t pushforward $X$)}
    \end{align*}
\end{proposition}

\begin{proposition}[expectation of product of two independent random variables]
    Let $X, Y$ be independent real-valued random variables, then
    $$
        \Exp[XY] = \Exp[X] \Exp[Y]
    $$
\end{proposition}

\begin{proposition}[inner product space]
    $\RV[\R]$ is an inner product space over $\R$ where the inner product is defined by
    $$
        \inner{X, Y} = \Exp[XY]
    $$
\end{proposition}

\begin{theorem}[Cauchy-Schwarz inequality]
	Since $\RV[\R]$ is an inner product space over $\R$, then if $X, Y$ are real-valued random variables, then
    $$
        \Exp[XY]^2 \leq \Exp[X^2] \Exp[Y^2]
    $$
\end{theorem}

\begin{theorem}[Markov inequality]
    Let $X: \Omega \to [0, +\infty)$, then for any $a > 0$, we have
    $$
        P(\set{X > a}) \leq \frac{\Exp[X]}{a}
    $$
\end{theorem}

\begin{proposition}[expectation as a sum of tail probabilities]
	If $X: \Omega \to \N$, then
	$$
		\Exp[X] = \sum_{n=0}^\infty n P(X = n) = \sum_{n=0}^\infty P(X > n)
	$$
	
	If $X: \Omega \to \R$, then
	$$
		\Exp[X] = \int_\R x  = \int_0^\infty P(X > a) da
	$$
\end{proposition}

\subsubsection{Variance of Real-Valued Random Variables}

\begin{definition}[variance, $p$-th moment]
    Let $X: \Omega \to \R$. Define $\Var: \RV[\R] \to \R$ by
    $$
        \Var(X) := \Exp[X^2] - \Exp[X]^2 = \Exp[(X - \Exp[X])^2]
    $$
    $\Var(X)$ is called variance of $X$, $\Exp[X^p]$ is called $p$-th moment of $X$, and $\Exp[|X|^p]$ is called $p$-th absolute moment of $X$
\end{definition}

\begin{definition}[Chebyshev inequality]
    Let $X: \Omega \to \R$, then for any $a > 0$
    $$
        P(\set{|X - \Exp[X]| > a}) \leq \frac{\Var(X)}{a^2}
    $$
\end{definition}

\begin{definition}[covariance, correlation]
    Let $X, Y$ be real-valued random variables. Define the covariance $\Cov: \RV[\R] \times \RV[\R] \to \R$ by
    $$
        \Cov(X, Y) := \Exp[XY] - \Exp[X]\Exp[Y] = \Exp[(X - \Exp[X]) (Y - \Exp[Y])]
    $$
    
    Define the correlation $\Corr: \RV[\R] \times \RV[\R] \to [-1, +1]$ by
    $$
    	\Corr(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X) \Var(Y)}}
    $$
\end{definition}

\begin{proposition}
	Given a collection $\set{X_1, X_2, ..., X_n}$ of real-valued random variables with finite second moments, i.e $\Exp[X_i^2] < \infty$, then
	$$
		\Var(X_1 + X_2 + ..., + X_n) = \sum_{i=1}^n \sum_{j=1}^n \Cov(X_i, X_j) = \sum_{i=1}^n \Var(X_i) + 2 \sum_{i, j \in [n] \times [n]: i < j} \Cov(X_i, X_j)
	$$
\end{proposition}

\note{START FROM HERE}


\subsection{Limit Theorems}

\begin{definition}[convergence]
    Let $(X_n)_{n \in \N}$ and $X$ be real-valued random variables defined on the same probability space $(\Omega, F, P)$ with probability distribution $(\nu_n)_{n \in \N}$ and $\nu$ respectively.
    \begin{enumerate}
    	\item $X_n \to X$ almost surely if there exists a subset $\Omega_1 \subseteq \Omega$ with $P(\Omega_1) = 1$ such that for all $\omega \in \Omega_1$, as $n \to \infty$
    	$$
			X_n(\omega) \to X(\omega)
    	$$
    	
        \item $X_n \to X$ in probability if for all $\eps > 0$, as $n \to \infty$
        $$
			P(|X_n - X| \geq \eps) \to 0
        $$

        \item $X_n \to X$ in distribution (or $\nu_v \to \nu$ weakly) if for all $a < b$ with $\nu(\set{a}) = \nu(\set{b}) = 0$
        $$
			\nu_n(a, b) \to \nu(a, b)
        $$
    \end{enumerate}
\end{definition}

\begin{remark}
    Some remarks on convergence
    \begin{enumerate}
        \item almost surely convergence $\implies$ convergence in probability $\implies$ convergence in distribution

        \item for any constant $c \in \R$, $X_n \to c$ in probability $\iff$ $X_n \to c$ in distribution.

        \item weak convergence of $\nu_n \to \nu$ is equivalent to
        $$
			\int_\R f(x) d \nu_n \to \int f(x) d \nu
        $$
        
        as $n \to \infty$ for all bounded (absolutely) continuous function $f: \R \to \R$. $f$ is called test function.

        \item $X_n \to X$ in distribution means exactly $\nu_n \to \nu$ weakly, and hence $(X_n)_{n \in \N}$ and $X$ need not to be defined on the same probability space.
    \end{enumerate}
\end{remark}

\begin{theorem}[weak law of large numbers]
    Let $(X_i)_{i \in \N}$ be a sequence of i.i.d (independent and identically distributed) real-valued random variables. Assume that the mean $\mu = \Exp[X_1]$ finite. Let $\sigma = \sqrt{\Var(X_1)}$ and $S_n = \sum_{i=1}^n X_i$. Then the empirical average $\frac{S_n}{n} \to \mu$ in probability, i.e. for all $\eps > 0$, as $n \to \infty$
    $$
        P\tuple*{\abs*{\frac{S_n}{n} - \mu} > \eps} \to 0
    $$

    
\end{theorem}

\begin{proof}
    \note{Assume $\sigma < \infty$} \footnote{this simple proof is only for the case of finite second moment}, by Chebyshev inequality for real-valued random variable $\frac{S_n}{n}$,
    \begin{align*}
        P\tuple*{\abs*{\frac{S_n}{n} - \mu} > \eps}
        &\leq \frac{1}{\eps^2} \Var\tuple*{\frac{S_n}{n}} \\
        &= \frac{1}{\eps^2} \Exp\bracket*{\tuple*{\frac{S_n}{n} - \mu}^2} \\
        &= \frac{1}{\eps^2} \Exp\bracket*{\tuple*{\frac{X_1 + ... + X_n}{n} - \mu}^2} \\
        &= \frac{1}{\eps^2} \Exp\bracket*{\tuple*{\frac{(X_1 -\mu) + ... + (X_n -\mu)}{n}}^2} \\
        &= \frac{1}{n^2 \eps^2} \Exp\bracket*{((X_1 -\mu) + ... + (X_n -\mu))^2} &\text{(linearity of expectation)}\\
        &= \frac{1}{n^2 \eps^2} \tuple*{\sum_{i=1}^n\Exp[(X_i - \mu)^2] + \sum_{(i, j) \in [n] \times [n]: i \neq j} \Exp[(X_i - \mu) (X_j - \mu)]} &\text{(linearity of expectation)}\\
        &= \frac{1}{n^2 \eps^2} \tuple*{\sum_{i=1}^n\Exp[(X_i - \mu)^2] + \sum_{(i, j) \in [n] \times [n]: i \neq j} \Exp[X_i - \mu] \Exp[X_j - \mu]} &\text{($X_i - \mu$ and $X_j - \mu$ are independent)}\\
        &= \frac{1}{n^2 \eps^2} \sum_{i=1}^n\Exp[(X_i - \mu)^2] \\
        &= \frac{\Var(X_1)}{n \eps^2} = \frac{\sigma^2}{n \eps^2} \to 0
    \end{align*}
\end{proof}

\begin{theorem}[strong law of large numbers]
    Let $(X_i)_{i \in \N}$ be a sequence of i.i.d real-valued random variables with finite mean $\mu: \Exp[X_1] \in \R$. Then, almost surely
    $$
        \tuple*{\frac{S_n}{n}}_{n \in \N} \to \mu
    $$
\end{theorem}

\begin{lemma}[Borel-Cantelli]
    Let $(\Omega, F, P)$ be a probability space and $A_n \in F$ is a sequence of events. Then
    \begin{enumerate}
        \item if $\sum_{i=1}^\infty P(A_n) < \infty$ then almost surely $A_n$ eventually stops occurring, i.e. there is $\Omega_0 \subseteq \Omega$ with $P(\Omega_0) = 1$ such that for all $\omega \in \Omega_0$, $\omega \notin A_n$ for all but finitely many $n$ \note{$P(A_n) \to 0$ as $n \to \infty$}

        \item if $(A_n)_{n \in \N}$ are independent and $\sum_{n=1}^\infty P(A_n) = \infty$ then almost surely $A_n$ occur infinitely often, i.e. there is $\Omega_0 \subseteq \Omega$ with $P(\Omega_0) = 1$ such that for all $\omega \in \Omega_0$, $\omega \notin A_n$ for finitely many $n$
    \end{enumerate}
\end{lemma}

\begin{proof}[Sketch proof of strong law of large numbers]
    We will show that for each $\eps > 0$, let $A_n$ be the event $\set*{\abs*{\frac{S_n}{n} - \mu} > \eps}$, then $\sum_{n=1}^\infty P(A_n) < \infty$. By Borel-Cantelli, for almost every $\omega \in \Omega$, the event $A_n$ eventually stops occurring, hence
    $$
        \limsup_{n \to \infty} \abs*{\frac{S_n(\omega)}{n} - \mu} \leq \eps
    $$
    for almost every $\omega \in \Omega$. Let $\Omega_\eps \subseteq \Omega$ be the set where it holds, $P(\Omega_\eps) = 1$. Choose a sequence $(\eps_i)_{i \in \N} \to 0$, take $\Omega_0 = \bigcap_{i=1}^\infty \Omega_{\eps_i}$
\end{proof}


\begin{theorem}[central limit theorem]
    Let $(X_i)_{i \in \N}$ be i.i.d random variables with finite mean $\mu$ and finite variance $\sigma^2$ and let $S_n = \sum_{i=1}^n X_i$ and $W_n = \frac{S_n - n \mu}{\sigma \sqrt{n}}$. Then $W_n$ converges in distribution to a standard Gaussian random variable $Z$, that is, for all $a < b$, as $n \to \infty$
    $$
        P(W_n \in [a, b]) \to P(Z \in [a, b]) = \int_a^b \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx
    $$
\end{theorem}

\begin{theorem}[Poisson limit theorem - law of small numbers]
    For $n \in \N$, let $X_1, X_2, ..., X_n$ be independent Bernoulli random variables with $P(X_i = 1) = \frac{\lambda}{n}$ for some $\lambda > 0$ modelling the occurrence of $n$ independent rare events
    Then, $S_n = \sum_{i=1}^n X_i$ is a random variable modelling the number of occurrences

    As $n \to \infty$, $S_n$ converges in distribution to $Pois(\lambda)$, i.e. for each $k = 0, 1, ...$, as $n \to \infty$ 
    $$
        P(S_n = k) \to \tuple*{e^{-\lambda} \frac{\lambda^k}{k!}}
    $$
\end{theorem}

\begin{proof}
    \begin{align*}
        P(S_n = k)
        &= {n \choose k} \tuple*{\frac{\lambda}{n}}^k \tuple*{1 - \frac{\lambda}{n}}^{n-k} \\
        &= \frac{n!}{k!(n-k)!} \tuple*{\frac{\lambda}{n}}^k e^{(n-k)\log \tuple*{1 - \frac{\lambda}{n}}} \\
        &= \tuple*{\frac{n(n-1)...(n-k-1)}{n^k}} \tuple*{\frac{\lambda^k}{k!} e^{(n-k)\tuple*{-\frac{\lambda}{n} + o\tuple*{\frac{\lambda}{n}}}}} \\
        &\to \frac{\lambda^k}{k!} e^{-\lambda}
    \end{align*}
\end{proof}

\begin{definition}[Fourier transform]
    Let $X$ be a real-valued random variable with probability distribution $\mu$. The Fourier transform of $X$ is also called its characteristic function, is defined by
    $$
        \phi(t) = \Exp[e^{itX}] = \int_\R e^{itx} d \mu(x)
    $$
\end{definition}

\begin{theorem}
    The Fourier transform satisfies the following properties
    \begin{enumerate}
        \item $\phi(t) \in \C$ with $|\phi(t)| \in [0, 1]$ for all $t \in \R$ and $\phi(0) = 1$

        \item $\phi$ determines the distribution of $X$ with $\phi^{(k)}(0) = i^k \Exp[X^k]$

        \item if $\phi_n(t) = \Exp[e^{it X_n}]$ and $\phi(t) = \Exp[e^{itX}]$, then $\phi_n \to \phi$ pointwise on $[-a, +a]$ for some $a > 0$ implies $X_n \to X$ in distribution
    \end{enumerate}
\end{theorem}

\begin{proof}[Proof of central limit theorem]
    Suppose $\mu = 0, \sigma = 1$, let $\psi(t) = \Exp[e^{it X_1}]$. Then $\psi(0) = 1, \psi'(0) = 0$ and $\psi''(0) = -1$, Taylor theorem,
    $$
        \psi(t) = 1 - \frac{t^2}{2} + o(t^2)
    $$
    
    where $\frac{o(f(\eps))}{f(\eps)} \to 0$ as $\eps \to 0$ for any function $f$
    For any $t \in \R$,
    \begin{align*}
        \phi_n(t)
        &= \Exp[e^{it W_n}] \\
        &= \Exp\bracket*{e^{i \frac{t}{\sqrt{n}} \sum_{i=1}^n X_i}} \\
        &= \Exp\bracket*{e^{i \frac{t}{\sqrt{n}} X_1} ... e^{i \frac{t}{\sqrt{n}} X_n}} \\
        &= \Exp\bracket*{e^{i \frac{t}{\sqrt{n}} X_1}} ... \Exp\bracket*{e^{i \frac{t}{\sqrt{n}} X_n}} &\text{($X_i$ are independent)} \\
        &= \psi \tuple*{\frac{t}{\sqrt{n}}}^n \\
        &= \tuple*{1 - \frac{t^2}{2n} + o \tuple*{\frac{1}{n}}}^n \\
        &= e^{n \log \tuple*{1 - \frac{t^2}{2n} + o \tuple*{\frac{1}{n}}}} \\
        &= e^{n \bracket*{ \tuple*{- \frac{t^2}{2n} + o \tuple*{\frac{1}{n}}} + o\tuple*{- \frac{t^2}{2n} + o \tuple*{\frac{1}{n}}}} } \\
        &\to e^{- \frac{t^2}{2}}
    \end{align*}

	Since $\Exp[e^{it Z}] = e^{- \frac{t^2}{2}}$ is the characteristic function of standard Gaussian $Z$. So, $W_n \to Z$ in distribution.
\end{proof}

\begin{definition}[Laplace transform]
    Let $X$ be non-negative real-valued random variable with probability distribution $\mu$. Then the Laplace transform of $X$ (or $\mu$) is defined to be
    $$
        \Lambda(\lambda) = \Exp[e^{-\lambda X}] = \int_0^\infty e^{-\lambda x} d \mu(x)
    $$
\end{definition}

\begin{definition}[generating function]
    Let $X$ be a non-negative $\N$-valued random variable with probability mass function $p_n = P(X = n)$ for $n \in \set{0, 1, ...}$. Then the generating function of $X$ (or $p_n$) is defined to be
    $$
        G(s) = \Exp[s^X] = \sum_{n=0}^\infty s^n p_n
    $$
    for $s \geq 0$ so that the sum converges
\end{definition}


\note{CONTINUE FROM HERE}


\section{Conditioning}

\subsection{Conditioning on Event}

\begin{definition}[conditioning on event, conditional probability, conditional probability space, conditional probability distribution]
    Let $(\Omega, \mathcal{F}, P)$ be a probability space and $E$ be an event such that $P(E) > 0$, then $E$ induces another probability space $(\Omega, \mathcal{F}, P(\cdot | E))$ where the probability measure $P(\cdot | E)$ is defined by
    $$
        P(A | E) = \frac{P(A \cap E)}{P(E)}
    $$
    $P(A | E)$ is call conditional probability, $P(\cdot|E)$ is called conditional probability measure and $(\Omega, \mathcal{F}, P(\cdot | E))$ is called conditional probability space. If $X: \Omega \to \mathcal{X}$ is a random variable, conditioning on event $E$ yields a new probability distribution, namely, conditional probability distribution $P_{X|E} = P(\set{X \in \cdot}|E): \mathcal{F}_X \to \R$
    $$
        P_{X|E}(E_X) = P(\set{X \in E_X} | E) = P(X^{-1} E_X | E) = \frac{P(X^{-1} E_X \cap E)}{P(E)}
    $$ 
\end{definition}

\begin{definition}[conditional expectation on event, conditional variance]
    Let $X$ be a real-valued random variable on $(\Omega, F, P)$ and $E$ is a event with $P(E) > 0$. Define the conditional expectation of $X$ conditioned by event $E$ by
    $$
        \Exp[X|E] = \int_\Omega X d P(\cdot|E) = \frac{\Exp[X 1_E]}{P(E)}
    $$
    Define the conditional variance of $X$ conditioned by event $E$ by
    $$
        \Var(X|E) = \Exp[X^2|E] - \Exp[X|E]^2 = \Exp[(X - \Exp[X|E])^2 | E]
    $$
\end{definition}

\subsection{Conditioning on Discrete Random Variable}

\begin{definition}[conditioning on discrete random variable]
    Let $X: \Omega \to \mathcal{X}$ be a random variable, $Y: \Omega \to \mathcal{Y}$ be a discrete random variable. Define $P(X|Y): \mathcal{Y} \to \Hom(\mathcal{F}_X, \R)$ as a function from value in $\mathcal{Y}$ to a distribution on $\mathcal{X}$ by
    \begin{align*}
    	P(X|Y): \mathcal{Y} &\to \Hom(\mathcal{F}_X, \R) \\
    									y &\mapsto P(\set{X \in \cdot} | \set{Y = y})
    \end{align*} 
\end{definition}

\begin{definition}[conditional expectation on discrete random variable]
	Let $f: \Hom(\mathcal{F}_X \to \R) \to \R$ be a real-valued function on distribution on $\mathcal{X}$ (e.g expectation, variance). Then, the composition of $f \circ P(X|Y)$ is a function $\mathcal{Y} \to \R$. When $f$ is expectation, we have
	\begin{align*}
		\Exp[X|Y]: \mathcal{Y} &\to \R \\
							y &\mapsto \Exp[X| \set{Y = y}]
	\end{align*}
\end{definition}

\begin{proposition}[tower property of conditional expectation]
	When $X$ is a real-valued random variables and $Y$ is a discrete random variable, let $A \subseteq \mathcal{X}$ be an event, then $P(X \in A)$ can be recovered from $P(X|Y)$ by
	$$
		P(X \in A) = \sum_{y \in \mathcal{Y}} P(X \in A | Y = y) P(Y = y) = \Exp_Y[P(X \in A | Y)]
	$$
	
	$\Exp[X]$ can be recovered from $\Exp[X|Y]: \mathcal{Y} \to \R$ by
	$$
		\Exp[X] = \sum_{y \in \mathcal{Y}} \Exp[X 1_{\set{Y = y}}] = \sum_{y \in \mathcal{Y}} \Exp[X|Y] P(Y=y) = \Exp[\Exp[X|Y]]
	$$
	
	More generally, let $f: \mathcal{X} = \R \to \R$, then
	$$
		\Exp[f(X)] = \sum_{y \in \mathcal{Y}} \Exp[f(X) 1_{\set{Y = y}}] = \sum_{y \in \mathcal{Y}} \Exp[f(X)|Y] P(Y=y) = \Exp[\Exp[f(X)|Y]]
	$$
	
	$P(X \in -)$ is a mixture of conditional distributions $P(X \in - | Y = y)$ with mixture coefficients $P(Y = y)$. $\Exp[X]$ is a mixture of conditional expectation $\Exp[X | Y=y]$ with mixture coefficients $P(Y=y)$
\end{proposition}

\begin{remark}[marginal distribution on many variables]
	Let $\phi(X, Y)$ be a function of random variables which is itself another random variable. We can write
	$$
		\Exp[\phi] = \Exp[\Exp[\phi | X]]
	$$
	
	Now, $\psi = \Exp[\phi | X]$ is a random variable, then
	$$
		\Exp[\phi] = \Exp[\Exp[\phi | X]] = \Exp[\psi] = \Exp[\Exp[\psi | Y]] = \Exp[\Exp[ \Exp[\phi | X] | Y]]
	$$
\end{remark}


\subsection{Conditioning on Continuous Random Variable}

\begin{definition}[conditioning on continuous random variable]
    When both $X, Y$ are continuous random variables on $\R$, define $P(X \in A | Y = y_0)$ and $E[X|Y=y_0]$ by the conditional density
    $$
        f_X(x|Y = y_0) = \frac{f(x, y_0)}{\int_\R f(x, y_0) dx}
    $$
\end{definition}

\subsection{Conditioning on $\sigma$-Algebra}

\begin{definition}[conditioning on $\sigma$-algebra]
    Let $\mathcal{G} \subseteq \mathcal{F}$ be a sub-$\sigma$-algebra \footnote{a $\sigma$-algebra that is a subset} in $(\Omega, \mathcal{F}, P)$ and $X: \Omega \to \mathcal{X}$ be a random variable. Define $P(X|\mathcal{G}): \mathcal{G} \to \Hom(\mathcal{F}_X, \R)$ as a function from events in $\mathcal{G}$ to a distribution on $\mathcal{X}$ by
    \begin{align*}
    	P(X | \mathcal{G}): \mathcal{G} &\to \Hom(\mathcal{F}_X, \R) \\
    	E_\mathcal{G} &\mapsto P(\set{X \in \cdot} | E_\mathcal{G})
    \end{align*}

	When $\mathcal{G}$ is countable (discrete random variable), the set function $P(X | \mathcal{G}): \mathcal{G} \to \Hom(\mathcal{F}_X, \R)$ can be characterized by a function
	\begin{align*}
		\Omega &\to \Hom(\mathcal{F}_X, \R) \\
		g &\mapsto P(\set{X \in \cdot} | G)
	\end{align*}
	
	where $G \in \mathcal{G}$ is the smallest set in $\mathcal{G}$ containing $g$, the smallest set in $\mathcal{G}$ containing $g$ is the intersection of all events in $\mathcal{G}$ containing $g$. This is coincide with the discrete random variable case.
\end{definition}

\begin{remark}[conditional expectation on $\sigma$-algebra]
	When $X$ is a real-valued random variable, we have the conditional expectation
	\begin{align*}
		\Exp[X | \mathcal{G}]: \mathcal{G} &\to \R \\
		G &\mapsto \Exp[X | G]
	\end{align*}
	
	If $\mathcal{G}$ is countable, then the set function $\Exp[X | \mathcal{G}]: \mathcal{G} \to \R$ can be characterized by a function
	\begin{align*}
		\Omega &\to \R \\
		g &\mapsto \Exp[X|G]
	\end{align*}
	
	where $G \in \mathcal{G}$ is the smallest set in $\mathcal{G}$ containing $g$
\end{remark}


\begin{proposition}[tower property of conditional expectation]
	Let $\mathcal{H} \subseteq \mathcal{G}$ be sub-$\sigma$-algebras in $(\Omega, \mathcal{F}, P)$ and $X: \Omega \to \R$ be a real-valued random variable. Then,
	$$
		\Exp[X | \mathcal{H}] = \Exp[\Exp[X | \mathcal{G}] | \mathcal{H}]
	$$
\end{proposition}

\chapter{Time-Homogeneous Markov Chain}

In this chapter, all Markov chains are time-homogeneous

\section{Markov Chain Basics}

\begin{definition}[time-homogeneous Markov chain]
    A stochastic process $X = (X_n)_{n \in \N_0}$ with countable state space $S$ is called a Markov chain if
    $$
        P(X_{n+1} = y | X_0 = x_0, ..., X_n = x_n) = P(X_{n+1} = y | X_n = x_n)
    $$

    for all $n \in \N_0$. If $P(X_{n+1} = y | X_n = x_n) = P(X_1 = y | X_0 = x)$ for all $n \in \N_0$, then $X$ is called time-homogeneous, in that case we define $\Pi: S \times S \to \R$
    $$
        \Pi(x, y) = P(X_1 = y | X_0 = x)
    $$

    $\Pi$ is called transition probability matrix. When $S$ is finite, $\Pi$ is a right stochastic matrix, that is, all elements are non-negative and sum of every row is $1$
\end{definition}

\begin{remark}[$n$-step]
	Let $\Pi^n: S \times S \to \R$ be defined by
	$$
		\Pi^n(x, y) = P(X_n = y | X_0 = x) = \sum_{(x = z_0, z_1, ..., z_n = y)} \Pi(z_0, z_1) \Pi(z_1, z_2) ... \Pi(z_{n-1}, z_n)
	$$
	
	where the sum is over all possible path $(x = z_0, z_1, ..., z_n = y)$ of length $n$. $\Pi^n$ is also a right stochastic matrix.
\end{remark}

\begin{proposition}[$\Pi$ acting on row vector]
    Let $X$ be a time-homogeneous Markov chain with countable state space $S$ and $\mu: S \to \R$ be a distribution for $X_0$, then the distribution of $X_n$, denoted by $\mu_n: S \to \R$ is
    $$
        \mu_n(y) = P(X_n = y) = \sum_{x \in S} P(X_n = y, X_0 = x) = \sum_{x \in S} \mu_0(x) \Pi^n(x, y)
    $$
    
    If $S$ is finite, we can write it in matrix form $\mu_n = \mu_0 \Pi^n$
    \begin{align*}
		\Hom(S, \R) \times \Hom(S \times S, \R) &\to \Hom(S, \R) \\
		(\mu_0, \Pi^n) &\mapsto \mu_n = \mu_0 \Pi^n
    \end{align*}
\end{proposition}

\begin{proposition}[$\Pi$ acting on column vector]
	Let $f: S \to \R$ be a function on random variable $X$ which is also a linear function on $S$. Let $\Pi^n f: S \to \R$ be defined by
	$$
		(\Pi^n f)(x) = \Exp[f(X_n) | X_0 = x]= \sum_{y \in S} \Pi^n(x, y) f(y)
	$$
	
	If $S$ is finite, we can write it in matrix form $\Pi^n f$
    
    \begin{align*}
    	\Hom(S \times S, \R) \times \Hom(S, \R)&\to \Hom(S, \R) \\
    	(\Pi^n, f) &\mapsto \Pi^n f
    \end{align*}
\end{proposition}

\begin{proposition}[stationary measure]
    Since $1: S \to \R$ is a right eigenvector of $\Pi$ with eigenvalue $1$, $\Pi$ also has a left eigenvector with eigenvalue $1$. In other words, there exists a measure $\nu: S \to \R$ such that $\nu \Pi = \nu$, such $\nu$ is called stationary measure
\end{proposition}
    
\begin{proposition}[spectrum of $\Pi$]
    All complex eigenvalues of $\Pi$ have norm less than or equal $1$
\begin{proof}
    Let $(\lambda, g)$ be a right eigenvalue eigenvector of $\Pi$ where $\lambda \in \C$, $g: S \to \C$. Suppose $S$ is finite and $|g(x)|$ achieves maximum at $x_0$, then
    $$
        |\lambda| |g(x_0)| = |\lambda g(x_0)| = |(\Pi g) (x_0)| = \abs*{\sum_{y \in S} \Pi(x_0, y) g(y)} \leq \abs*{\sum_{y \in S} \Pi(x_0, y)} |g(x_0)| = |g(x_0)|
    $$
    That is, $|\lambda| \leq 1$
\end{proof}
\end{proposition}

\begin{proposition}[intercommunicating states]
    Let $S$ be the state space, two states $x, y \in S$ are called intercommunicate, denoted by $x \sim y$ if there exist $m, n \in \N_0$ such that
    \begin{align*}
        P(X_m = y | X_0 = x) &> 0 \\
        P(X_n = x | X_0 = y) &> 0
    \end{align*}
    
    The relation is an equivalence relation that partitions the state space $S$ into equivalence classes of intercommunicating.
\end{proposition}

\begin{definition}[irreducible Markov chain]
    A time-homogeneous Markov chain is irreducible if $S$ is a single equivalence class with respect to intercommunicating intercommunicating equivalence relation $\sim$
\end{definition}

\section{Recurrence Transience}

\begin{remark}[notation]
	Given a stochastic process $X = (X_0, X_1, ...)$, let $E$ be any event and $Y$ be a random variable, then we write
	\begin{align*}
		P_x(E) &= P(E | X_0 = x) \\
		\Exp_x[Y] &= \Exp[Y | X_0 = x]
	\end{align*}
\end{remark}

\begin{definition}[$T_x^k$, $f_{xy}$, $N_x$, $G(x, y)$]
    Let $X$ be a stochastic process with countable state space $S$, define the following:
    \begin{enumerate}
    	\item Let $T_x^0 = 0$, for any $k > 0$, let $T_x^k$ be the random variable modelling the time when $X$ visits $x$ at the $k$-th time, that is,
    	$$
    		T_x^k = \min \set{n \in \N_0: n > T_y^{k-1}, X_n = x}
    	$$
    	
    	\item Let $f_{xy}$ be the probability that $X$ visits $y$ in finite time given $X_0 = x$, that is,
    	$$
    		f_{xy} = P_x(T_y^1 < \infty)
    	$$
    	
    	\item Let $N_x$ be the random variable modelling the number of visiting state $x$, that is
    	$$
    		N_x = \sum_{n=0}^\infty 1_{\set{X_n = x}}
    	$$
    	
    	\item Let $G(x, y)$ be the expected number of visiting $y$ given $X_0 = x$, that is
    	$$
    		G(x, y) = \Exp_x[N_y]
    	$$
    \end{enumerate}
\end{definition}

\begin{definition}[recurrence transience]
    Let $X$ be a stochastic process with countable state space $S$. A state $x \in X$ is called recurrent if $f_{xx} = 1$ and called transient if $f_{xx} < 1$
\end{definition}

\begin{proposition}[expected number of visits for irreducble Markov chain]
    Let $X$ be a time-homogeneous irreducible Markov chain with countable state space $S$. Let $X_0 = x$, the the expected number of visiting $x$ is
    $$
        G(x, x) = \frac{1}{1 - f_{xx}}
    $$
\begin{proof}
    We will show that $N_x$ is a geometric random variable with parameter $p = 1 - f_{xx}$. For each $k > 0$, we have
    \begin{align*}
        P_x(N_x = k) &= P_x(T_x^{k-1} < \infty, T_x^k = \infty) \\
        P_x(N_x \geq k) &= P_x(T_x^{k-1} < \infty)
    \end{align*}

    We have
    \begin{align*}
        &P_x(N_x \geq k) - P_x(N_x \geq k+1) \\
        &= P_x(N_x = k) \\
        &= P_x(T_x^{k-1} < \infty, T_x^k = \infty) \\
        &= \sum_{n=0}^\infty P_x(T_x^{k-1} = n, T_x^k = \infty) \\
        &= \sum_{n=0}^\infty P_x(T_x^{k-1} = n) P_x(T_x^k = \infty| T_x^{k-1} = n) \\
        &= \sum_{n=0}^\infty P_x(T_x^{k-1} = n) P_x(T_x^1 = \infty| T_x^0 = n) &\text{(time-homogeneous)} \\
        &= \sum_{n=0}^\infty P_x(T_x^{k-1} = n) P(T_x^1 = \infty| X_0 = x) \\
        &= \sum_{n=0}^\infty P_x(T_x^{k-1} = n) (1 - f_{xx}) \\
        &= (1 - f_{xx}) \sum_{n=0}^\infty P_x(T_x^{k-1} = n) \\
        &= (1 - f_{xx}) P_x(T_x^{k-1} < \infty) \\
        &= (1 - f_{xx}) P_x(N_x \geq k)
    \end{align*}

    Therefore,
    $$
        P_x(N_x \geq k) = f_{xx}^k \text{ and } P_x(N_x = k) = f_{xx}^k (1 - f_{xx})
    $$
\end{proof}
\end{proposition}

\begin{proposition}[recurrence transience as a class property]
    Let $x, y \in S$ be two inter-communicating states. Then, $x, y$ are either both transient or both recurrent
\begin{proof}
    It suffices to show that $G(x,x) = \infty$ if and only if $G(y,y) = \infty$. By assuming $x \sim y$, there exists $k, l \in \N$ such that
    $$
        \Pi^k(x, y) > 0 \text{ and } \Pi^l(y, x) > 0
    $$

    Note that, for all $n \in \N_0$, we have the probability of starting from $x$ then coming back to $x$ after $k+n+l$ steps is greater than probability of starting from $x$, going to $y$ in $k$ steps, staying in $y$ in $n$ steps, then coming back to $x$ in $l$ steps, that is
    $$
        \Pi^{k+l+n}(x,x) \geq \Pi^k(x,y) \Pi^n(y,y) \Pi^l{y,x}
    $$

    Summing over $n \in \N_0$, we have
    \begin{align*}
        G(x,x)
        &= \Exp_x\bracket*{\sum_{m=0}^\infty 1_{\set{X_m = x}}} \\
        &= \sum_{m=0}^\infty \Pi^m(x,x) \\
        &\geq \sum_{m=k+l}^\infty \Pi^m(x,x) \\
        &\geq \sum_{n=0}^\infty \Pi^k(x,y) \Pi^n(y,y) \Pi^l(y,z) \\
        &= \Pi^k(x,y) \Pi^l(y,z) \sum_{n=0}^\infty \Pi^n(y,y) \\
        &= \Pi^k(x,y) \Pi^l(y,z) G(y,y) \\
    \end{align*}

    Therefore, $G(y,y) = \infty$ implies $G(x,x) = \infty$
\end{proof}
\end{proposition}

\begin{definition}[recurrent transient Markov chain]
    A time-homogeneous irreducible Markov chain with countable state space is called recurrent/transient if its states are recurrent/transient
\end{definition}

\begin{corollary}[irreducible finite state Markov chain]
    A time-homogeneous irreducible Markov chains with finite state space are recurrent
\begin{proof}
    Since Markov chain is finite, there is at least one state with expected number of visits being infinity, that state is recurrent. Moreover, irreducibility implies every other state is inter-communicating with the recurrent state, therefore, all states are recurrent.
\end{proof}
\end{corollary}

\begin{proposition}
    Let $X$ be a time-homogeneous irreducible Markov chain with countable state space $S$. Then,
    \begin{enumerate}
        \item If $X$ is recurrent, then $P(N_x = \infty) = 1$ for all $x \in S$ and $G(x, y) = \infty$ for all $x, y \in S$.
        \item If $X$ is transient, then $P(N_x < \infty) = 1$ for all $x \in S$ and $G(x, y) < \infty$ for all $x, y \in S$
    \end{enumerate}
\begin{proof}
    Since the distribution of $X = (X_n)_{n \in \N_0}$ is a mixture with different starting position, i.e. for any event $E$
    $$
        P(E) = \sum_{x \in S} \mu(x) P_x(E)
    $$

    It suffices to prove for the case when $X$ starts from any state $s \in S$.
    
\begin{enumerate}
    \item $X$ is recurrent
    
    For any $y \in S$, since $X$ returns to $x$ infinitely many times, let $X_n = x$, by irreducibility, there exists $m \in \N$ such that
    $$
        P(X_{n+m} = y | X_n = x) = \Pi^m(x, y) \geq 0
    $$

    Therefore, everytime $X$ visits $x$, there is a positive probability $X$ visits $y \in S$, that is, visiting $y$ is a sequence of i.i.d Bernoulli random variables of positive parameter. Hence, number of visits $y$ is infinite. Then, $G(x, y) = \infty$

    \item $X$ is transient
    
    Probability of starting from $x$, going to $y$ in $m$ steps, then going back to $x$ in $p$ steps is less than probability of starting from $x$ and going back to $x$ in $n = m+p$ steps.
    
    \begin{align*}
        G(x, x) 
        &= \sum_{n=0}^\infty \Pi^n(x, x) \\
        &\geq \sum_{n=m}^\infty \Pi^n(x, x) \\
        &\geq \sum_{p=0}^\infty \Pi(x, y)^m \Pi^p(y, x) \\
        &= \Pi(x, y)^m \sum_{p=0}^\infty  \Pi^p(y, x) \\
        &= \Pi(x, y)^m G(y, x) \\
    \end{align*}

    Hence, $G(x, x) $ is finite implies $G(y, x)$ is finite. $G(y, x)$ finite implies $P_y(N_x = \infty) = 0$
\end{enumerate}
\end{proof}
\end{proposition}

\begin{remark}[transient - escape to infinity]
    In the transient case, $X$ escape to infinity with probability $1$ in the following sense: For any finite set of states $F$, with probability $1$
    $$
        \max\set{n \in \N: X_n \in F} < \infty
    $$
\end{remark}

\begin{proposition}[escaping from a finite set]
    Let $X$ be an irreducible Markov chain with countable state space $S$. Let $F \subseteq S$ be a finite, and $T_{F^c} = T_{F^c}(X) = \min\set{n \geq 0: X_n \notin F}$ be the first time $X$ exits from $F$. Then there exists $C > 0$ and $\rho \in (0, 1)$ such that for all $n \in \N_0$ and all initial distributions
    $$
        P(T_{F^c}(X) > n) \leq C \rho^n
    $$
\begin{proof}
    Let $\rho \in [0, 1]$ be defined by
    $$
        \rho = \max \set{P(X_1 = y | X_0 = x): y \in F, x \in F}
    $$

    We can assume $\rho < 1$ since if $P(X_1 = y | X_0 = x) = 1$, as $F$ is finite, we can merge two states $x, y$ into a new state and the merging process terminates with $\rho < 1$ and a finite set of states $F$. Then
    \begin{align*}
        P(T_{F^c}(X) > n)
        &= P(X_0 \in F, X_1 \in F, ..., X_n \in F) \\
        &= P(X_0 \in F) \prod_{i=0}^n P(X_{i+1} \in F | X_i \in F) \\
        &\leq P(X_0 \in F) \rho^{n-1}
    \end{align*}
\end{proof}
\end{proposition}

\begin{theorem}[PÃ³lya 1921]
    The symmetric random walk on $\Z^d$ is recurrent in dimension $d=1, 2$ and transient in $d \geq 3$
\begin{proof}
    \note{TODO}
\end{proof}
\end{theorem}

\note{CONTINUE FROM HERE}

\section{Stationary Measure}

\begin{proposition}[limiting distribution of transient Markov chain]
    Let $X$ be an irreducible transient Markov chain. Then for all $x, y \in S$,
    $$
        \Pi^n(x, y) \to 0
    $$

    as $n \to \infty$. Consequently, for any initial distribution, $P(X_n = y) \to 0$ for all $y \in S$
\begin{proof}
    Since $X$ is transient, for any $x, y \in S$
    $$
        G(x, y) = \sum_{n=0}^\infty \Pi^n(x, y) < \infty
    $$

    Then, $\Pi^n(x, y) \to 0$ as $n \to \infty$.
\end{proof}
\end{proposition}

\begin{definition}[stationary distribution, stationary measure]
    Let $X$ be a Markov chain with countable state space $S$ and transition matrix $\Pi$. A probability distribution $\mu$ on $S$ is called stationary distribution for $X$ if
    $$
        \sum_{x \in S} \mu(x) \Pi(x, y) = \mu(y)
    $$

    for all $y \in S$. That is, $\mu \Pi = \mu$. In other words, if $X_0$ has distribution $\mu$, then $X_1$ also has distribution $\mu$, hence so do $X_2, X_3, ...$. In general, any $\nu: S \to [0, +\infty)$ with $\nu \Pi = \nu$ and $\sum_{x \in S} \nu(x) \in (0, +\infty]$ is called stationary measure for $X$
\end{definition}

\section{Positive Recurrence, Null Recurrence, Existence of Stationary Measure}

\begin{definition}[positive recurrent Markov chain, null recurrent Markov chain]
    Let $X$ be an irreducible Markov chain with countable state space $S$. Let $T_x = T_x^1$,  we call $X$ positive recurrent if $\Exp_x[T_x] < \infty$ for all $x \in S$ and null recurrent if $\Exp_x[T_x] = \infty$ for all $x \in S$.
\end{definition}

\begin{proposition}[positive recurrence and positive recurrence as a class property]
    If $x$ and $y$ are two intercommunicating recurrent states, then they are either both positive recurrent or both null recurrent.
\begin{proof}
    \note{TODO}
\end{proof}
\end{proposition}

\begin{theorem}[existence of stationary measure for recurrence Markov chain]
    Let $X$ be an irreducible recurrent Markov chain with state space $S$. For each $x \in S$, 
    $$
        \nu(y) = \Exp_x\bracket*{\sum_{n=0}^{T_x - 1} 1_{\set{X_n = y}}} = \Exp_x\bracket*{\sum_{n=0}^\infty 1_{\set{X_n = y}}1_{\set{n < T_x}} } = \sum_{n=0}^\infty \Exp_x[1_{\set{X_n=y, n < T_x}}] = \sum_{n=0}^\infty P_x(X_n=y, n < T_x)
    $$
    
    is a stationary measure. If $X$ is positive recurrent, then we can normalize $\nu$ to a stationary distribution.
\begin{proof}
	The technique is called cycle trick (\note{need to redo}). It suffices to show that $\sum_{z \in S} \nu(z) \Pi(z, y) = \nu(y)$ for all $y \in S$. We have

	Case 1: $y \neq x$
	
	\begin{align*}
		&\nu(y) \\
		&= \sum_{n=0}^\infty P_x(X_n=y, n < T_x) \\
		&= \sum_{n=1}^\infty P_x(X_n=y, n < T_x) &\text{($P_x(X_0=y, 0 < T_x) = 0$)} \\
		&= \sum_{n=1}^\infty \sum_{z \in S} P_x(X_{n-1} = z, X_n=y, n < T_x) \\
		&= \sum_{z \in S} \sum_{n=1}^\infty P_x(X_{n-1} = z, X_n=y, n < T_x) &\text{(Tonelli theorem)} \\
		&= \sum_{z \in S} \sum_{n=1}^\infty P_x(X_{n-1} = z, X_n=y, n - 1 < T_x) &\text{($\set{X_n=y, n < T_x} = \set{X_n=y, n-1 < T_x}$)}\\
		&= \sum_{z \in S} \sum_{n=1}^\infty P_x(X_{n-1} = z, n - 1 < T_x) P_x(X_n = y | X_{n-1} = z, n-1 < T_x)\\
		&= \sum_{z \in S} \sum_{n=1}^\infty P_x(X_{n-1} = z, n - 1 < T_x) P_x(X_n = y | X_{n-1} = z) &\text{($\set{n-1 < T_x} = \set{X_0 \neq x, .., X_{n-1} \neq x}$)}\\
		&= \sum_{z \in S} \Pi(z, y) \sum_{n=1}^\infty P_x(X_{n-1} = z, n - 1 < T_x) \\
		&= \sum_{z \in S} \Pi(z, y) \nu(z) \\
	\end{align*}
	
	Case 2: $y = x$

	\begin{align*}
		&\nu(x) \\
		&= \sum_{n=0}^\infty P_x(X_n=x, n < T_x) \\
		&= 1 \\
		&= \sum_{n=1}^\infty P_x(n = T_x) \\
		&= \sum_{n=1}^\infty \sum_{z \in S} P_x(X_{n-1}=z, n = T_x) \\
		&= \sum_{z \in S} \sum_{n=1}^\infty P_x(X_{n-1}=z, n = T_x) &\text{(Tonelli theorem)}\\
		&= \sum_{z \in S} \sum_{n=1}^\infty P_x(X_{n-1}=z, X_n=x, n-1 < T_x) &\text{($\set{X_n=x, n-1 < T_x} = \set{n < T_x}$)}\\
		&= \sum_{z \in S} \sum_{n=1}^\infty P_x(X_{n-1}=z, n-1 < T_x) P_x(X_n=x | X_{n-1}=z, n-1 < T_x) \\
		&= \sum_{z \in S} \sum_{n=1}^\infty P_x(X_{n-1}=z, n-1 < T_x) P_x(X_n=x | X_{n-1}=z) &\text{($\set{n-1 < T_x} = \set{X_0 \neq x, .., X_{n-1} \neq x}$)}\\
		&= \sum_{z \in S} \Pi(z, x) \sum_{n=1}^\infty P_x(X_{n-1}=z, n-1 < T_x) \\
		&= \sum_{z \in S} \Pi(z, x) \nu(z)
	\end{align*}

	Hence, $\nu$ is stationary.
\end{proof}
\end{theorem}

\begin{theorem}[uniqueness of stationary measure for recurrent Markov chain]
    Let $X$ be a recurrent Markov chain and $\nu: S \to \R$ be defined by
    $$
        \nu(y) = \Exp_x\bracket*{\sum_{n=0}^{T_x - 1} 1_{\set{X_n = y}}} = \sum_{n=0}^\infty P_x(X_n=y, n < T_x)
    $$

    Then if $\Tilde{\nu}: S \to \R$ is another stationary measure for $X$, then there exists $C \in \R$ such that $\Tilde{\nu}(y) = C \nu(y)$ for all $y \in S$
\begin{proof}
    Without loss of generality, assume $\Tilde{\nu}(x) = 1$ we will prove that $\Tilde{\nu}(y) = \nu(y)$. By stationary of $\Tilde{\nu}$, we have
    $$
        \Tilde{\nu}(y) = \sum_{z_1 \in S} \Tilde{\nu}(z_1) \Pi(z_1, y) = \Pi(x, y) + \sum_{z_1 \neq x} \Tilde{\nu}(z_1) \Pi(z_1, y)
    $$

    Apply the same decomposition for $z_1$
    \begin{align*}
        &\Tilde{\nu}(y) \\
        &= \Pi(x, y) + \sum_{z_1 \neq x} \Tilde{\nu}(z_1) \Pi(z_1, y) \\
        &= \Pi(x, y) + \sum_{z_1 \neq x} \tuple*{\Pi(x, z_1) + \sum_{z_2 \neq x} \Tilde{\nu}(z_2) \Pi(z_2, z_1)} \Pi(z_1, y) \\
        &= \Pi(x, y) + \sum_{z_1 \neq x} \Pi(x, z_1) \Pi(z_1, y) + \sum_{z_1 \neq x} \sum_{z_2 \neq x} \Tilde{\nu}(z_2) \Pi(z_2, z_1) \Pi(z_1, y) \\
        &= \Pi(x, y) + P_x(X_2 = y, X_1 \neq x) + \sum_{z_1 \neq x} \sum_{z_2 \neq x} \Tilde{\nu}(z_2) \Pi(z_2, z_1) \Pi(z_1, y) \\
    \end{align*}

    \note{TODO - finish this later}
\end{proof}
\end{theorem}


\section{Long Time Limit of Markov Chain}

\begin{remark}
    For transient Markov chain, we have shown that $\Pi^n(x, y) = P_x(X_n = y) \to 0$ as $n \to \infty$ for all $x, y \in S$
\end{remark}

\begin{definition}[period of an irreducible Markov chain]
    Let $X$ be an irreducible Markov chain with state space $S$ and transition matrix $\Pi$. The period $r_x \in \N$ of a state $x$ is defined by
    $$
        \gcd \set{n \in \N: \Pi^n(x, x) > 0}
    $$

    It can be shown that $r_x = r_y$ for all $x, y \in S$. Hence, the period $r \in \N$ of $X$ is defined by the period of any $x \in S$. A Markov chain is called periodic if $r \geq 2$ and aperiodic if $r = 1$
\end{definition}

\begin{remark}[cyclic structure of periodic Markov chain]
    For a periodic Markov chain with period $r$, we can divide the state space $S$ into $r$ equivalence classes $S_1, S_2, ..., S_r$. Let $i \in [r]$, for any state in $S_i$, the only transition possible is to another state in $S_{i+1}$ (where $S_{r+1} = S_1$). If we define $Y_n = X_{nr}$, then $Y$ is an aperiodic Markov chain with state space $S_i$ where $Y_0 \in S_i$. Therefore, any periodic Markov chain can be broken down to aperiodic Markov chains.
\end{remark}

\begin{theorem}[long time limit of aperiodic positive recurrent Markov chain]
    Let $X$ be a aperiodic positive recurrent Markov chain with state space $S$ and transition matrix $\Pi$. Let $\mu$ denote the unique stationary distribution. Then for any initial distribution $\mu_0$, $X_n$ converges to $\mu$ in distribution, that is
    $$
        P_{\mu_0}(X_n = y) \to \mu(y)
    $$

    as $n \to \infty$ for all $y \in S$
\begin{proof}
    \note{TODO- coupling}
\end{proof}

\end{theorem}

\begin{theorem}[long time limit of null recurrent Markov chain]
    Let $X$ be a null recurrent Markov chain with state space $S$, then
    $$
        \Pi^n(x, y) \to 0
    $$

    as $n \to \infty$ for any $x, y \in S$. Hence, for any initial distribution $\mu_0$,
    $$
        P_{\mu_0}(X_n = y) \to 0
    $$

    as $n \to \infty$ for any $y \in S$
\end{theorem}

\section{Renewal Process}

\begin{definition}[discrete renewal process]
    A discrete renewal process $\tau$ is a sequence of $\N_0$-valued random variable $(\tau_n)_{n \in \N_0}$ where $\tau_0 = 0$ and $(\tau_n - \tau_{n-1})_{n \in \N}$ are i.i.d $\N \cup \set{\infty}$-valued random variables with probability mass function $f(k) = P(\tau_1 = k)$ for $k \in \N \cup \set{\infty}$. That is, the distribution of increments is fixed.
\end{definition}

\begin{remark}
    The natural interpretation of $(\tau_n)_{n \in \N_0}$ is the collection of times when we change the light bulb such that light bulbs have i.i.d random lifetimes with probability mass function $f$
\end{remark}

\begin{remark}
    Given a Markov chain $(X_n)_{n \in \N_0}$ with $X_0 = x$, the sequence $T_x^m$ for $m \in \N_0$ is a renewal process where $f(\infty) > 0$ if and only if $x$ is transient.
\end{remark}

\begin{remark}[discussion on the Markov chain of renewal process]
    $\Pi(n, n-1) = 1$, $\Pi(0, k-1) = f(k)$ 
\end{remark}

\begin{theorem}[renewal theorem]
   Let $\tau$ be a discrete renewal process, if $\tau$ is transcient, that is, $f(\infty) > 0$ or null recurrent that is $f(\infty) = 0$ and $\sum_{k \in \N} k f(k) = \infty$, then
   $$
   		P(n \in \tau) = P(n \in \set{\tau_1, \tau_2, ...}) \to 0
   $$
   
   as $n \to \infty$. If $\tau$ is positive recurrent that is $f(\infty) = 0$ and $\sum_{k \in \N} k f(k) < \infty$, and $\tau$ is aperiodic, that is, $r = \gcd(n: f(n) > 0) = 1$, then
   $$
   		P(n \in \tau) = P(n \in \set{\tau_1, \tau_2, ...}) \to \frac{1}{\sum_{k \in \N} k f(k)}
   $$
   
   as $n \to \infty$
\end{theorem}

\section{Reversible Measure, Reversible Markov Chain}

\begin{definition}[reversible measure, reversible Markov chain]
    Let $X$ be a Markov chain with state space $S$ and transition matrix $\Pi$. A measure $\nu: S \to \R$ is a reversible measure of $X$ if
    $$
        \nu(x) \Pi(x, y) = \nu(y) \Pi(y, x)
    $$

    for all $x, y \in S$. The condition is called detailed balance. A Markov chain is called reversible if it has a reversible measure.
\end{definition}

\begin{remark}
    A reversible measure $\nu$ must be stationary, since
    $$
        \nu(x) = \sum_{y \in S} \nu(x) \Pi(x, y) = \sum_{y \in S} \nu(y) \Pi(y, x)
    $$

    If we interpret a distribution as the distribution of masses over all states, then each time step, masses are transferred. Stationary means for each state, the in-mass equals the out-mass. Reversibility means for each pair of state $x, y$, the mass $x \to y$ equals the mass $y \to x$
\end{remark}

\begin{proposition}[time reversibility]
    Let $\nu$ be a reversible distribution of a Markov chain $X$. If $X_0$ has distribution $\nu$, then $(X_0, ..., X_n)$ has the same distribution as its time reversal $(X_n, ..., X_0)$, that is
    $$
        P_\nu(X_0=x_0, ..., X_n = x_n) = P_\nu(X_0=x_n, ..., X_n = x_0)
    $$

    Moreover, if given a stationary measure $\nu$, $(X_0, ..., X_n)$ has the same distribution as its time reversal $(X_n, ..., X_0)$, then $\nu$ is reversible.
\begin{proof}
    \note{TODO}
\end{proof}
\end{proposition}

\begin{theorem}[loop condition for reversibility]
    An irreducible Markov chain is reversible if and only if the transition matrix $\Pi$ satisfies the loop condition, that is, given $x \in S$,
    $$
        \frac{\Pi(x_0, x_1)}{\Pi(x_1, x_0)} ... \frac{\Pi(x_{n-1}, x_n)}{\Pi(x_n, x_{n-1})} = 1
    $$

    for all path $(x = x_0, x_1, ..., x_{n-1}, x_n = x)$. In that case, we can construct a stationary measure by
    $$
        \nu(y) = \nu(x) \frac{\Pi(y_0, y_1)}{\Pi(y_1, y_0)} ... \frac{\Pi(y_{n-1}, y_n)}{\Pi(y_n, y_{n-1})}
    $$

    for a path $(y = y_0, y_1, ..., y_{n-1}, y_n = x)$
\begin{proof}
    \note{TODO}
\end{proof}
\end{theorem}

\begin{remark}[reversible Markov chain as random walk on electric network]
	Any reversible MC can be seen as a random walk on a graph $G = (V, E)$ with $V = S$ and $(x, y) \in E$ if $\Pi(x, y) > 0$ with conductance $C(x, y)$ where
	$$
		C(x, y) = \nu(x) \Pi(x, y)
	$$
	
	with $\nu: S \to \R$ is a stationary measure
	$$
		\nu(x) = \sum_{(x, z) \in E} C(x, z)
	$$
\end{remark}

\section{Hitting Probability, Expected Hitting Time}

Given a Markov chain $X$ with state space $S$, let $A, B \subseteq S$ be two disjoint subsets of $S$, let $T_A = \min(n \geq 0: X_n \in A)$, $T_B = \min(n \geq 0: X_n \in B)$ be the first times the Markov chain visiting $A$ and $B$.

\subsection{Hitting Probability}

Let
$$
	f(x) = P_x(T_A < T_B)
$$

be the probability of hitting $A$ before $B$ . The boundary conditions are for every $x \in A$, $f(x) = 1$, for every $x \in B$, $f(x) = 0$. If $x \notin A \cup B$, then
\begin{align*}
	f(x)
	&= P_x(T_A < T_B) \\
	&= \sum_{y \in S} P_x(X_1 = y, T_A < T_B) &\text{(one step analysis)} \\
	&= \sum_{y \in S} P_x(X_1 = y) P_x(T_A < T_B | X_1 = y) \\
	&= \sum_{y \in S} P_x(X_1 = y) P_y(T_A < T_B) \\
	&= \sum_{y \in S} \Pi(x, y) f(y) = (\Pi f)(x)
\end{align*}

Hence, $(\Pi - I) f = 0$. $f$ is called a harmonic function of the operator $\Pi$. (\note{related to Laplace equation})

\subsection{Expected Hitting Time}

Let 
$$
	g(x) = \Exp_x[T_A]
$$

be the expected hitting time for $A$. The boundary condition is for every $x \in A$, $g(x) = 0$. If $x \notin A$, then
\begin{align*}
	g(x)
	&= \Exp_x[T_A] \\
	&= \Exp_x\bracket*{\sum_{y \in S} T_A 1_{\set{X_1 = y}}} \\
	&= \Exp_x\bracket*{1 + \sum_{y \in S} (T_A - 1) 1_{\set{X_1 = y}}} \\
	&= 1 + \sum_{y \in S} \Exp_x[(T_A - 1) 1_{\set{X_1 = y}}] \\
	&= 1 + \sum_{y \in S} P_x(X_1 = y) \Exp_x[T_A - 1 | X_1 = y] \\
	&= 1 + \sum_{y \in S} P_x(X_1 = y) \Exp_y[T_A] \\
	&= 1 + \sum_{y \in S} \Pi(x, y) g(y) = 1 + (\Pi g)(x)
\end{align*}

Hence, $(\Pi - I) g = 1$ (\note{related to Poisson equation})

\section{Monte Carlo, Metropolis, Gibbs sampling}

\note{SKIP - NOT IN EXAM}

\chapter{Martingale}

\section{Martingale basics}

\begin{definition}[$\sigma$-algebra filtration]
	A filtration on $(\Omega, F, P)$ is an increasing sequence of $\sigma$-algebras $(F_n)_{n \geq 0}$ with 
	$$
		F_0 \subseteq F_1 \subseteq ...
	$$
\end{definition}

\begin{remark}
	Let $X = (X_n)_{n \in \N_0}$ be a stochastic process, then the filtration defined by $F_n = \sigma(X_0, X_1, ..., X_n)$ is called the canonical filtration generated by $X$
\end{remark}

\begin{definition}[martingale]
	Given a filtration $G_0 \subseteq G_1 \subseteq G_2 \subseteq ...$, a real-valued stochastic process $X = (X_n)_{n \in \N_0}$ is called a martingale adapted to the filtration $G_\cdot$ if
	\begin{enumerate}
		\item For all $n \in \N_0$, $\Exp[|X_0|] < \infty$ and $\Exp[X_n | G_n] = X_n$. That is, $G_n$ contains all information of $X_n$, $\sigma(X_n) \subseteq G_n$
		\item For all $n \in \N_0$, $\Exp[X_{n+1} | G_n] = X_n$. This is the notion of fair game, that is, given the past information ($G_n$), the expectation ($\Exp[X_{n+1} - X_n | G_n]$) of $X_{n+1} - X_n$ is zero.
	\end{enumerate}
	
	\note{we have, $\Exp[X_{n+2} | G_{n+1}] = X_{n+1}$, then $X_n = \Exp[X_{n+1} | G_n] = \Exp[\Exp[X_{n+2} | G_{n+1}] | G_n] = \Exp[X_{n+2} | G_n]$. hence, for any $n < m$, then $\Exp[X_m | G_n] = X_n$}
\end{definition}

\begin{remark}[sub-martingale, super-martingale]
	If we replace the second condition for martingle by $\Exp[X_{n+1} | G_n] \geq X_n$, it is called sub-martingale and $\Exp[X_{n+1} | G_n] \leq X_n$, it is called super-martingale
\end{remark}
 
 \subsection{Doob Decomposition, Doob Martingale}

Given a stochastic process $(X_n)_{n \in \N_0}$ and let $F_n = \sigma(X_0, X_1, ..., X_n)$ be the canonical filtration generated by $X$. Let $D_n = X_n - X_{n-1}$. Then, let $M_0 = 0$ and
$$
	M_n = M_{n-1} + D_n - \Exp[D_n | F_{n-1}] = \sum_{i=1}^n (D_i - \Exp[D_i |F_{i-1}])
$$ 

\begin{proposition}
	$(M_n)_{n \in \N_0}$ is a martingale	
\end{proposition}

Let $A_n = \sum_{i=1}^n \Exp[D_i |F_{i-1}]$, note that, $A_n$ is not a random variable but a sequence of real numbers

\begin{theorem}[Doob decomposition]
	Every stochastic process $(X_n)_{n \in \N_0}$ can be decomposed into
	$$
		X_n = X_0 + M_n + A_n
	$$
	
	where $M_n$ is a martingale and $A_n$ is a sequence of real numbers.
\end{theorem}

\begin{proposition}[Doob martingale, martingale decomposition]
	If $Y$ is a random variable and $G_0 \subseteq G_1 \subseteq G_2 \subseteq ...$ is a filtration in $(\Omega, F, P)$, then
	$
		Z_n = \Exp[Y | G_n]
	$
	is a martingale. This is a direct application of tower property
	$$
		\Exp[Z_n | G_{n-1}] = \Exp[\Exp[Y | G_n] | G_{n-1}] =  \Exp[Y | G_{n-1}] = Z_{n-1}
	$$
	
	If $G_n = F$, then $Z_0 = \Exp[Y]$ and $Z_n = Y$, martingale decomposition
	$$
		Y = \Exp[Y] + (Z_n - Z_0) = \Exp[Y] = \sum_{i=1}^n (Z_i - Z_{i-1})
	$$
\end{proposition}




\note{TODO - generalize this}

\subsection{Martingale in Markov chain}

Let $X$ be a Markov chain with state space $S$ and transition matrix $\Pi$. Let $f: S \to \R$ be a bounded function on $S$, then Doob decomposition gives
$$
	f(X_n) = f(X_0) + M_n + A_n
$$

where $M_n$ is a martingale adapted to the canonical filtration of $X$ and
\begin{align*}
	A_n
	&= \sum_{i=1}^n \Exp[f(X_i) - f(X_{i-1}) | F_{i-1}] \\
	&= \sum_{i=1}^n \Exp[f(X_i) - f(X_{i-1}) | X_0, X_1, ..., X_{i-1}] \\
	&= \sum_{i=1}^n \Exp[f(X_i) - f(X_{i-1}) | X_{i-1}] \\
	&= \sum_{i=1}^n \Exp[f(X_i)| X_{i-1}] - f(X_{i-1}) \\
	&= \sum_{i=1}^n (\Pi - I) f (X_{i-1}) \\
\end{align*}

Hence, if $(\Pi - I) f = 0$ ($f$ is harmonic) then $f(X_n)$ is a martingale, if $(\Pi - I) f = -1$, then $f(X_n) + n$ is a martingale. 

Now let $A, B \subseteq S$ be disjoint and $T_A = \set{n \geq 0: X_n \in A}$ be the first time hitting $A$, we want to compute
$$
	f(x) = P_x(T_A < T_B) \text{ and } g(x) = \Exp_x[T_A]
$$

Through one step analysis, we have shown that
$$
	(\Pi - I) f = 0 \text{ and } (\Pi - I) g = 1
$$

Then, $f(X_n)$ is a martingale before time $T_A \wedge T_B = \min\set{T_A, T_B}$, $g(X_n) + n$ is a martingale before time $T_A$

\section{Azuma-Hoeffding Inequality}

\begin{theorem}[Azuma-Hoeffding]
	Let $(X_n)_{0 \leq n \leq N}$ be a martingale with $X_0$ and its increments $D_i = X_i - X_{i-1}$ satisfy $|D_i| \leq K$ for all $1 \leq i \leq N$ almost surely (true for a set $\Omega_0 \subseteq \Omega$ of realizations with $P(\Omega_0) = 1$). Then, for all $a > 0$,
	$$
		P\tuple*{\frac{X_N}{\sqrt{N}} \geq + a} \leq e^{- \frac{a^2}{2K}} \text{ and } P\tuple*{\frac{X_N}{\sqrt{N}} \leq - a} \leq e^{- \frac{a^2}{2K}}
	$$
\begin{proof}
	\note{TODO}
\end{proof}
\end{theorem}

\note{TODO - generalize to the case where $|D_i| \leq K_i$}


\section{Stopped Martingale}

\begin{definition}[stopping time]
	A random variable $\tau$ on $\N_0 \cup \set{\infty}$ is called stopping time with respect to the filtration $(F_n)_{n \in \N_0}$ if $\set{\tau = n} \in F_n$ for all $n \geq 0$ (\note{I am kinda get it but not really get it. at least I don't do probability so just know enough to pass the exam}). $\tau$ models the stopping time, that is, to decide when to stop a martingale, we only have the information available up to that time. (sub-$\sigma$-algebra is information)
\end{definition}

\begin{proposition}
	If $\tau_1$ and $\tau_2$ are stopping time with respect to the filtration $(F_n)_{n \in \N_0}$, then $\tau_1 \wedge \tau_2 = \min\set{\tau_1, \tau_2}$ and $\tau_1 \vee \tau_2 = \max\set{\tau_1, \tau_2}$ are stopping times.
\end{proposition}

\begin{definition}[stopped $\sigma$-field \footnote{$\sigma$-algebra is also called $\sigma$-field}]
	Let $\tau$ be a stopping time with respect to the filtration $(F_n)_{n \geq 0}$, the stopped $\sigma$-field $F_\tau$ associated with the stopping time $\tau$ is defined by
	$$
		F_\tau = \set{A \in F: A \cap \set{\tau = n} \in F_n \text{ for all } n \geq 0}
	$$
	
	that is, the collection of measurable events $A$ in which we can determine whether it will occur or not based on the available information up time time $\tau$
\end{definition}

\begin{lemma}[stopped martingale is a martingale]
	Let $(X_n)_{n \geq 0}$ be a martingale adapted to a filtration $(F_n)_{n \geq 0}$ and $\tau$ be a stopping time with respect to $(F_n)_{n \geq 0}$. Then $Y_n = X_{n \wedge \tau}$, the martingale $X_n$ stopped at time $\tau$, is also a martingale with respect to $(F_n)_{n \geq 0}$. More generally, if $\theta$ is another stopping time with $\theta \leq \tau$ almost surely, then $X_{n \wedge \tau} - X_{n \wedge \theta}$ is also a martingale.
\begin{proof}
	\note{TODO}
\end{proof}
\end{lemma}

\subsection{Upcrossing Inequality, Martingale Convergence Theorem, Backward Martingale}

\begin{definition}[upcrossing]
	Let $(X_n)_{n \geq 0}$ be a super-martingale adapted to the filtration $(F_n)_{n \geq 0}$. An upcrossing by $X$ of the interval $(a, b)$ with $a < b$ consists of a pair of times $k < l$ with $X_k \leq a$ and $X_l \geq b$. Let $U_n$ be the number of complete upcrossings $X$ makes before (\note{before and at}) time $n$ and define 
	\begin{align*}
		\tau_1 &= \min\set{i \geq 0: X_i \leq a} \\
		\tau_2 &= \min\set{i \geq \tau_1: X_i \geq b} \\
		&... \\
		\tau_{2k + 1} &= \min\set{i \geq \tau_{2k}: X_i \leq a} \\
		\tau_{2k + 2} &= \min\set{i \geq \tau_{2k+1}: X_i \geq b} \\
	\end{align*}
	
	where the minimum of an empty set is taken to be $\infty$.  Note that, $\tau_i$ is a stopping time and $U_n = \max\set{k: \tau_{2k} \leq n}$
\end{definition} 

\begin{lemma}[upcrossing inequality]
	Let $(X_n)_{n \geq 0}$ be a super-martingale and $U_n$ be the number of complete upcrossings over $(a, b)$ before time $n$, then
	$$
		\Exp[U_n] \leq \frac{\Exp[(a - X_n)^+]}{b-a} \leq \frac{|a| + \Exp[|X_n^-|]}{b - a}
	$$
	
	where $x^+ = \max\set{x, 0}$, $x^- = \min\set{x, 0}$
\begin{proof}
	\note{TODO}
\end{proof}
\end{lemma}

\begin{theorem}[martingale convergence theorem]
	If $(X_n)_{n \geq 0}$ is a super-martingale and $\sup_{n \in \N} \Exp[|X_n^-|] < \infty$, then there exists a random variable $X_\infty$ such that almost surely $X_n \to X_\infty$ as $n \to \infty$ and $\Exp[|X_\infty|] < \infty$. For sub-martingale, the condition is $\sup_{n \in \N} \Exp[|X_n^+|] < \infty$.
\begin{proof}
	\note{TODO}
\end{proof}
\end{theorem}

\begin{corollary}
	If $(X_n)_{n \geq 0}$ is a non-negative super-martingale then $X_\infty = \lim_{n \to \infty} X_n$ exists almost surely and $\Exp[X_\infty] \leq \Exp[X_0]$
\end{corollary}

\begin{corollary}
	Let $(X_n)_{n \geq 0}$ be a martingale with $|X_{n+1} - X_n| \leq M < \infty$ almost surely for all $n \geq 0$, then almost surely either $\lim_{n \to \infty} X_n$ exists and finite or $\limsup_{n \to \infty} X_n = +\infty$ and $\liminf_{n \to \infty} X_n = - \infty$. That is, either $X_n$ converges or osscilates between $-\infty$ and $+\infty$ 
\end{corollary}

\begin{definition}[backward martingale]
	$(X_n)_{n \geq 0}$ is called a backward martingale adapted to the decreasing filtration $F_0 \supseteq F_1 \supseteq ...$ if 
	$$
		\Exp[X_n | F_{n+1}] = X_{n+1}
	$$
	
	Note that, $X_n = \Exp[X_0 | F_n]$ for all $n \geq 0$, and $(..., X_2, X_1, X_0)$ is a martingale adapted to the filtration $... \subseteq F_2 \subseteq F_1 \subseteq F_0$
\end{definition}

\begin{theorem}
	Let $(X_n)_{n \geq 0}$ be a backward martingale adapted to a decreasing filtration $(F_n)_{n \geq 0}$, then almost surely $X_n \to X_\infty$ and $\Exp[X_\infty] = \Exp[X_0]$
\end{theorem}

\begin{lemma}[when does martingale limit preserve mean]
	If for some $K > 0$, the martingale $(X_n)_{n \geq 0}$ is bounded, that is, $P(|X_n| \leq K) = 1$ for all large $n$, then almost surely $X_n \to X_\infty$ and $\Exp[X_\infty] = \Exp[X_0]$
\begin{proof}
	\note{TODO}
\end{proof}
\end{lemma}

\section{Uniform Integrable Martingale, Optional Stopping Theorem}

\begin{definition}[uniform integrability]
	A sequence of random variables $(X_n)_{n \geq 0}$ is called uniformly integrable if for each $\eps > 0$, there exists $K > 0$ such that
	$$
		\sup_{n \geq 0} \Exp[|X_n| 1_{|X_n| > K}] \leq \eps
	$$
\end{definition}

\begin{remark}[$L^p$ ($p > 1$) implies uniformly integrable]
	If $\sup_{n} \Exp[|X_n|^p] < \infty$ for some $p > 1$, then Markov inequality implies that $(X_n)_{n \geq 0}$ is uniformly integrable.
\end{remark}

\begin{theorem}
	Let $(X_n)_{n \geq 0}$ be a martingale that is uniformly integrable, then almost surely $X_n \to X_\infty$ and $\Exp[X_\infty] = \Exp[X_0]$
\begin{proof}
	\note{TODO}
\end{proof}
\end{theorem}

\begin{theorem}[optional stopping theorem]
	Let $(X_n)_{n \geq 0}$ be a martingale and $\tau$ a finite stopping time adapted to the same filtration $(F_n)_{n \geq 0}$. If the sequence $(X_{n \wedge \tau})_{n \geq 0}$ is uniformly integrable, then $\Exp[X_\tau] = \Exp[X_0]$ ($X_\infty = X_\tau$)
\end{theorem}

\begin{remark}
	Doob martingale is uniformly integrable. If $X_n$ is a martingale with $\Exp[|X_n|] < \infty$, then for any convex function $\phi$, $\phi(X_n)$ is a sub-martingale.
\end{remark}

\section{Doob Maximal Inequality}

\begin{theorem}[Doob maximal inequality]
	Let $(X_i)_{i \in \N}$ be a sub-martingale with respect to filtration $(F_i)_{i \in \N}$. Let $S_n = \max_{1 \leq i \leq n} X_i$ be the running maximum of $X_i$, then for any $l > 0$,
	$$
		P(S_n \geq l) \leq \frac{1}{l} \Exp[X_n^+ 1_{\set{S_n \geq l}}] = \frac{1}{l} \Exp[X_n^+]
	$$
	
	where $X_n^+ = X_n \vee 0 = \max \set{X_n, 0}$. In particular, if $(X_i)_{i \in \N}$ is a martingale and the absolute value function is convex, then $|X_i|$ is a sub-martingale, then let $M_n = \max_{1 \leq i \leq n} |X_i|$,
	$$
			P(M_n \leq l) \leq \frac{1}{l} \Exp[|X_n| 1_{\set{M_n \leq l}} \leq \frac{1}{l} \Exp[|X_n|]
	$$
\end{theorem}

\begin{corollary}
	For any $p > 1$, $x \mapsto (x^+)^p$ and $x \mapsto |x|^p$ are convex functions, then
	\begin{align*}
		P(S_n \leq l) &\leq \frac{1}{l^p} \Exp[(X_n^+)^p 1_{\set{S_n \geq l}}] \leq \frac{1}{l^p} \Exp[(X_n^+)^p] \\
		P(M_n \leq l) &\leq \frac{1}{l^p} \Exp[|X_n|^ 1_{\set{S_n \geq l}}] \leq \frac{1}{l^p} \Exp[|X_n|^p]
	\end{align*}
	
	where $X_n$ being sub-martingale and martingale correspondingly.
\end{corollary}

\begin{theorem}[Doob $L^p$ maximal inequality]
	For any $p > 1$,
	\begin{align*}
		\Exp[(S_n^+)^p] &\leq \tuple*{\frac{p}{p-1}}^p \Exp[(X_n^+)^p] \\
		\Exp[M_n^p] &\leq \tuple*{\frac{p}{p-1}}^p \Exp[|X_n|^p]
	\end{align*}
\end{theorem}

\section{Square-Integrable Martingale and Quadratic Variation}

\begin{definition}[square-integrable martingale, quadratic variation process]
	A martingale $(X_n)_{n \geq 0}$ is called square-integrable if $\Exp[X_n^2] < \infty$. If $(X_n)_{n \geq 0}$ is a square-integrable martingale, then $X_n^2$ is a sub-martingale with Doob decomposition
	$$
		X_n^2 = M_n + \inner{X}_n
	$$
	
	where $M_n$ is a martingale and $\inner{X}_n$ is a monotone increasing sequence
	$$
		\inner{X}_n = \sum_{n = 2}^n \Exp[(X_i - X_{i-1})^2 | F_{i-1}]
	$$
	
	$\inner{X}_n$ is called quadratic variation process of $X$
\end{definition}

\begin{theorem}
	Let $(X_n)_{n \in \N}$ be a square-integrable martingale and $\inner{X}_n$ its quadratic variation process. Then
	\begin{enumerate}
		\item on the event $\set{\inner{X}_\infty < \infty}$, almost surely $\lim_{n \to \infty} X_n$ exists and finite.
		
		\item on the event $\set{\inner{X}_\infty = \infty}$, almost surely $\lim_{n \to \infty} \frac{X_n}{\inner{X}_n} = 0$ 
	\end{enumerate}
\end{theorem}

\section{Martingale from Change of Measure}

\note{SKIP - I JUST DON'T WANT TO DO THIS }

\end{document}
